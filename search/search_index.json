{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#mobile-robots-oficial-site","title":"Mobile Robots Oficial Site","text":"<p>Oficial site for the Mobile Robotics research group of the RoboticsLab belonging to the University Carlos III of Madrid.</p>"},{"location":"index.html#welcome-to-the-mobile-robots-laboratory","title":"Welcome to the Mobile Robots Laboratory","text":"<p>In the Mobile Robots Group, we focus on developing robotics projects applied to real-world tasks, aiming to unify some of the main research areas in robotics, such as navigation, vision, manipulation, and grasping. To achieve this, we develop our own algorithms in this lab that allow us to operate in real environments, taking into account the surrounding elements and ensuring safe operations to assist people. Our main contributions are focused on indoor environments, specifically for tasks that help the elderly or dependent individuals. To carry out these tasks, we have our own platform, ADAM (Ambidextrous Domestic Autonomous Manipulator), on which we base all our work and test it in real settings.</p> <p> </p>"},{"location":"index.html#latest-news","title":"Latest News","text":"<ul> <li>24/09/2024: We have updated the ADAM section with the latest information on the robot's capabilities and the new features that have been added to it.</li> <li>04/09/2024 The Robotics Group assisted to the Jornadas de Autom\u00e1tica 2024 conference, where we presented our latest research on the development of a modular gripper for our robot ADAM and an innovative learning-from-demonstration technique for sweeping tasks with limited human data. The papers are:</li> <li>Educational and research platform with a modular parallel jaw gripper.</li> <li>Learning from demonstration through synthetic data for parameterized tasks.</li> </ul>"},{"location":"index.html#group-of-researchers","title":"Group of researchers","text":"<p>The Mobile Robots Group is composed of a team of researchers with different backgrounds and expertise in robotics. The team is composed by the members presented in about us.</p> <p> </p>"},{"location":"index.html#if-you-have-any-doubts-or-comments","title":"If you have any doubts or comments","text":"<p>In case of any questions or improvements to be made to this repository, feel free to comment on a new issue.</p>"},{"location":"index.html#license","title":"License","text":""},{"location":"ADAM/ADAM_v1.0.html","title":"Explain ADAM v1.0","text":""},{"location":"ADAM/ADAM_v2.0.html","title":"Explain ADAM v2.0","text":""},{"location":"ADAM/AdamOver.html","title":"Overview","text":""},{"location":"ADAM/AdamOver.html#adam-ambidextrous-domestic-autonomous-manipulator","title":"ADAM (Ambidextrous Domestic Autonomous Manipulator)","text":"<p>ADAM is a robot designed to perform everyday tasks for people who have difficulties carrying out such tasks. To achieve this, ADAM is equipped with various sensors and actuators that allow it to understand its surroundings and interact with them, giving it direct utility in the actions it can perform. For these processes, ADAM aims to be a development and coordination platform between different modules that enable its use in novel environments. The main research areas with ADAM are:</p> <p>Navigation: ADAM is capable of moving autonomously in indoor environments, avoiding obstacles and reaching the desired location. The main lines of research are:</p> <ul> <li> <p>Localiziation: ADAM is equipped with sensors that allow it to know its position in the environment.</p> </li> <li> <p>Base movement: ADAM is capable of navigate through different environments using information from various levels of abstraction, such as geometric information, topological information, and semantic information.</p> </li> </ul> <p>Vision: ADAM is equipped with cameras and LIDAR sensors that allow it to perceive its surroundings and interact with them.</p> <ul> <li> <p>Environment Detection: ADAM is equipped with vision algorithms based on CNN (Convolutional Neural Networks) that allow it to detect different classes of objects for real-time tracking.</p> </li> <li> <p>Synthetic Image Generation: ADAM has been endowed with an \"imagination\" capability that enables it to create new scenes without user interference.</p> </li> </ul> <p>Manipulation: ADAM has two robotic arms that allows it to interact with objects in its environment.</p> <ul> <li> <p>Learning for Manipulation: ADAM utilizes various Learning from Demonstration (LfD) algorithms that allow it to learn directly from human information to solve everyday tasks. For this purpose, various algorithms based on different LfD techniques have been developed to directly deploy on the ADAM robot and learn to perform manipulation tasks using human information.</p> </li> <li> <p>Human Tracking Algorithms: ADAM is equipped with a human tracking algorithm that enables it to follow human movements and adjust its own movements in the most optimal way possible. It also has other algorithms that enable it to autonomously learn to break down complex tasks into simpler ones.</p> </li> </ul> <p>Grasping: ADAM has a gripper and a robotic hand that allows it to grasp objects and manipulate them.</p> <ul> <li> <p>Manipulator Claws: ADAM has two custom-developed claws designed for gripping processes, equipped with a graphical interface and force feedback sensors.</p> </li> <li> <p>Optimal Grasping Algorithms: Currently, optimal grasping algorithms are being developed for ADAM, taking into account the shape and pose of the object to be grasped. For this process, various types of algorithms and techniques have been developed, based on classical optimization processes as well as learning techniques.</p> </li> </ul> <p>Simulation ADAM is equipped with a simulation environment that allows it to test its algorithms in a virtual environment before deploying them in the real world.</p> <ul> <li>Domestic Environment Simulation: ADAM has been implemented in CoppeliaSim for tasks requiring the simulation of domestic environments for synthetic dataset generation or indoor navigation testing.  </li> <li>PyBullet Simulator: A Bullet-based simulator is being developed to realistically simulate the ADAM robot and establish a connection with the physical robot.</li> </ul> <p>Hardware developments ADAM has various modifications in its design to add functionalities as well as additional hardware components that enhance its performance.</p> <p></p>"},{"location":"ADAM/AdamOver.html#versions-of-adam","title":"Versions of ADAM","text":"<ul> <li>ADAM v1.0: Our first aproximation, used until September 2024. The robot in which the first algorithms and tests were made.</li> <li>ADAM v2.0: New adaptation developed to solve some design issues and to improve the robot's capabilities. Model used from September 2024 onwards.</li> </ul>"},{"location":"ADAM/AdamOver.html#projects-developed-with-adam","title":"Projects developed with ADAM","text":"<ul> <li> <p>HEROITEA: Heterogeneous IntelligentMulti-Robot Team for Assistance of Elderly People (RTI2018- 095599-B-C21), funded by Spanish Min-isterio de Economia y Competitividad </p> </li> <li> <p>RoboCity2030-DIH-CM, Madrid Robotics Digital InnovationHub, S2018/NMT-4331, funded by \u201cProgramas de Actividades I+D en la Comunidad de Madrid\u201d and co-funded by Structural Funds of the EU.</p> </li> <li> <p>Advanced Mobile dual arm manipulator for Elderly People Attendance (AMME) (PID2022-139227OB-I00) by Ministerio de Ciencia e Innovaci\u00f3n, and by the National Science Foundation (FRR-2237463).</p> </li> </ul>"},{"location":"Grasping/GraspOver.html","title":"In work :construction_worker: :construction:","text":""},{"location":"Grasping/LfdGrasp.html","title":"Grasping with Learning from Demonstration","text":""},{"location":"Grasping/LfdGrasp.html#grasping-with-lfd-algorithms","title":"Grasping with LfD algorithms","text":""},{"location":"Grasping/LfdGrasp.html#imitation-learning-for-low-cost-dexterous-hand-using-rgbd-camera","title":"Imitation Learning for Low-Cost Dexterous Hand Using RGBD Camera","text":"<p>Multi-finger dexterous hands provide robots with a way to perform manipulation tasks in environments adapted for human beings. Their complexity makes the control process a challenging field of research. Techniques such as Deep Reinforcement Learning (DRL) aim to resolve their control but encounter problems and inefficiencies due to the large number of Degrees of Freedom (DoF) and the system's complexity. This is where the use of Imitation Learning (IL) techniques arises, which rely on human data to perform a learning process that optimizes the search space for an optimal solution. In this work, an IL application is presented for a low-cost dexterous robotic hand, the LEAP Hand. For this purpose, a data collection algorithm is used through the control of a generic robotic hand that adapts to the user's structure and allows the generalization of these demonstrations to any robotic hand. We have implemented a data capture process using an RGBD RealSense camera, which allows controlling the robotic hand. After this, we generate the data structure following a Markov Decision Process (MDP) through a Behavioral Cloning (BC) process, which is used as a basis for training the specific task. For this process, Demo Augmented Policy Gradient (DAPG) is used. To verify the efficiency, the data were collected by different users, and a comparison was made with highly used algorithms within the DRL field, obtaining more robust results in a shorter learning time.</p> <p></p> <p>The algorithm focuses on combining an imitation learning process with a reinforcement learning process. These policies are merged using a weight that determines, in each case, which information to prioritize for correctly solving the task. This results in a combined policy capable of performing the task by finding a balance between exploration and exploitation of prior information.</p> <p></p>"},{"location":"Grasping/LfdGrasp.html#papers-and-publications","title":"Papers and Publications","text":"<ul> <li>Imitation Learning for Low-Cost Dexterous Hand Using RGBD Camera</li> </ul>"},{"location":"Hardware/demo.html","title":"In work :construction_worker: :construction:","text":""},{"location":"Manipulation/datacq.html","title":"Data Acquisition","text":""},{"location":"Manipulation/datacq.html#data-acquisition-methods","title":"Data Acquisition Methods","text":"<p>In order to carry out the learning from demonstrations process, it is necessary to have processes that are able to collect data directly from the user. With this data, the necessary learning processes can be carried out to apply to manipulation tasks with our robotic platform ADAM. In the following, we present the work focused on the field of data acquisition for the training of the different algorithms created in our laboratory.</p>"},{"location":"Manipulation/datacq.html#taichi-tracking-algorithm-for-imitation-of-complex-human-inputs","title":"TAICHI (Tracking Algorithm for Imitation of Complex Human Inputs)","text":"<p>This project presents the TAICHI algorithm. This algorithm is focused on the tracking of people to collect data for the Imitation Learning technique. TAICHI is composed of a detection of the significant points of the human arm and its extraction and mapping to the robot, a Gaussian filtering process to smooth the movements and filter the sensor noise and an optimization algorithm that seeks to obtain through the inverse kinematics of the model the configuration that is closest to the human one and that does not generate collisions with its environment or with itself. The algorithm follows the following workflow:</p> <p></p> <p>The method allows data to be taken on the simulated ADAM model and subsequently evaluated on the real robot. The algorithm can be used with a single RGBD camera and by users of different ages and builds. The algorithm has been tested with a variety of users and has shown good results in the tracking of the human arm and the generation of trajectories that can be used for imitation learning.</p> <p></p>"},{"location":"Manipulation/datacq.html#papers-and-publications","title":"Papers and Publications","text":"<ul> <li> <p>TAICHI: Tracking Algorithm for Imitation of Complex Human Inputs</p> </li> <li> <p>Visual human tracking applied to non-anthropomorphic manipulators for imitation</p> </li> </ul>"},{"location":"Manipulation/datacq.html#creation-of-movement-primitives-libraries-for-lfd","title":"Creation of Movement Primitives Libraries for LfD","text":"<p>Motion primitives are a highly useful and widely employed tool in the field of Learning from Demonstration (LfD). However, obtaining a large number of motion primitives can be a tedious process, as they typically need to be generated individually for each task to be learned. To address this challenge, this work presents an algorithm for acquiring robotic skills through automatic and unsupervised segmentation. The algorithm divides tasks into simpler subtasks and generates motion primitive libraries that group common subtasks for use in subsequent learning processes. Our algorithm is based on an initial segmentation step using a heuristic method, followed by probabilistic clustering with Gaussian Mixture Models. Once the segments are obtained, they are grouped using Gaussian Optimal Transport on the Gaussian Processes (GPs) of each segment group, comparing their similarity through the energy cost of transforming one GP into another. This process requires no prior knowledge, is entirely autonomous, and supports multimodal information. The algorithm enables the generation of trajectories suitable for robotic tasks, establishing simple primitives that encapsulate the structure of the movements to be performed. Its effectiveness has been validated in manipulation tasks with a real robot, as well as through comparisons with state-of-the-art algorithms.</p> <p></p> <p>To perform the segmentation process, the algorithm relies on an initial heuristic approach followed by a probabilistic approximation. First, since heuristic methods tend to over-segment trajectories, we use this approach to generate initial segments. The connection points of these segments are then used as initialization points for the Gaussian Mixture Model, which re-connect the segments based on their probabilistic weights across all segments. For the clustering process, the algorithm utilizes a similarity cost between the generated segments. A Gaussian Process is employed to establish the relationship between each segment. To evaluate the cost, a Gaussian Optimal Transport (GOT) process is used to assess the cost of transforming one segment into another. The lower the transformation cost, the more similar the segments are. Consequently, if the similarity exceeds a certain threshold, the segments are considered similar. A visual example is presented below, where the same color represents the same movement library.</p> <p></p>"},{"location":"Manipulation/fml.html","title":"Fast Marching Learning Methods","text":""},{"location":"Manipulation/fml.html#fast-marching-learning-methods","title":"Fast Marching Learning Methods","text":"<p>To perform manipulation processes of environmental elements, different methods of Learning from Demonstration based on the ideas of Fast Marching have been developed. To achieve this, the generated algorithms are capable of utilizing velocity matrices to create a navigable environment through a series of initial demonstrations, which allow the algorithm to generate viable solutions for the execution of the manipulation tasks in question.</p>"},{"location":"Manipulation/fml.html#fast-marching-learning","title":"Fast Marching Learning","text":"<p>This algorithm combines kinesthetic teaching with the Fast Marching method to enable robots to learn manipulation tasks effectively. This approach allows robots to autonomously learn how to execute tasks based on human demonstrations, and it is particularly useful for complex manipulation scenarios where the robot must navigate and manipulate objects in dynamic environments. The process begins with the human demonstrator guiding the robot through the desired task. This kinesthetic teaching phase allows the robot to learn from the movements of the demonstrator, who physically moves the robot\u2019s arm along the desired path. These movements are captured as trajectories, which represent the way the robot should move in order to accomplish the task. The robot learns to replicate the demonstrated task through these trajectories, and these serve as the foundational input for the learning algorithm. Once the initial demonstrations are recorded, the algorithm employs the Fast Marching Square (FMS) method. The FMS method is a numerical approach commonly used in robotics to find the optimal path through an environment. It works by solving the Eikonal equation over a grid, where each point on the grid has a value associated with it. The Fast Marching algorithm propagates values from the source (the initial demonstrations) through the grid, generating a map that indicates the shortest paths to other points in the environment. This method is particularly suited for manipulation tasks as it helps the robot navigate through complex, obstacle-filled environments while maintaining efficiency and accuracy.</p> <p></p> <p>The robot uses the data from the FMS method to generate a navigable environment. It learns to understand the underlying dynamics of the task by analyzing the grid values and optimizing the movement paths based on the initial demonstrations. The Fast Marching method enables the robot to calculate the optimal path to follow in order to successfully complete the task, taking into account the positions of obstacles and the required movements to manipulate objects. A critical innovation in this algorithm is the auto-learning functionality, which allows the robot to explore unknown states of the environment. This exploration is essential for ensuring that the robot can adapt to dynamic and changing conditions. As the robot learns from the initial demonstrations, it begins to generalize its knowledge, enabling it to handle variations in the task environment. This exploration feature expands the robot\u2019s capacity to perform tasks beyond the scope of the initial demonstrations, ensuring that it can autonomously tackle new, unforeseen scenarios.</p> <p></p>"},{"location":"Manipulation/fml.html#papers-and-publications","title":"Papers and Publications","text":"<ul> <li>Kinesthetic Learning Based on Fast Marching Square Method for Manipulation</li> </ul>"},{"location":"Manipulation/fml.html#elastic-fast-marching-learning","title":"Elastic Fast Marching Learning","text":"<p>In this algorithm, we present the code for a novel method for learning skills from human demonstrations. Our method, Elastic Fast Marching Learning (EFML), combines ideas from Elastic Maps--a Learning from Demonstration (LfD) method based on a mesh of springs--and Fast Marching Learning--an LfD method based on velocity fields. The integration of these methods, inspired by real-world physical phenomena, allows a robot to generate reproductions with many desirable properties. These properties include the ability to be trained with a single or multiple demonstrations, adapting to any number of initial, final, or via-point constraints, and generating smooth reproductions. Additionally, we show that our integration works in both orientation space and task space, which has previously been unexplored by either method. The proposed algorithm displays advantages in terms of precision, smoothness, and speed.</p> <p></p> <p>The combination of both methods allows us to perform task learning while considering limitations in Cartesian space and also adding constraints for orientations within that space. This enables us to include mandatory waypoints, such as via-points, and also take into account the obstacles in the environment, which are characterized as areas of zero velocity, where the robot cannot generate a possible solution.</p> <p></p>"},{"location":"Manipulation/gmm.html","title":"Gaussian Mixtures Learning Methods","text":""},{"location":"Manipulation/gmm.html#gaussian-mixture-models-methods","title":"Gaussian Mixture Models Methods","text":"<p>Gaussian Mixture Models (GMMs) are widely used in LfD to enable robots to learn complex manipulation tasks from human demonstrations. GMMs provide a probabilistic framework to model the demonstrated trajectories or motion patterns by capturing their underlying structure as a mixture of Gaussian distributions. This flexibility allows the representation of variability and uncertainty in the demonstrations, which is essential for adapting learned behaviors to new situations. When applied to robotic manipulators, GMMs can model multi-dimensional motion trajectories, encompassing both spatial and temporal characteristics. By combining GMMs with algorithms like Gaussian Mixture Regression (GMR), the learned motion can be generalized to new scenarios, enabling robots to replicate human-like movements while maintaining precision and adaptability. This approach has been successfully applied in tasks such as pick-and-place operations, assembly, and tool use, where the ability to encode and reproduce complex motions is crucial.</p>"},{"location":"Manipulation/gmm.html#f-divergence-optimization-for-task-parameterized-learning","title":"f-Divergence Optimization for Task-Parameterized Learning","text":"<p>In this project, we present a demonstration-based learning algorithm based on TPGMM and optimization through the use of an f-Divergence model (specifically, we will use the Kullback-Leibler). This algorithm features a detector for irrelevant frames to generalize a learned policy for tasks not demonstrated by users. The algorithm relies on detecting redundant and irrelevant frames and filtering them out. To achieve this, the algorithm iteratively obtains the probabilistic values of an initial solution considering all frames as relevant. Based on these values, it autonomously iterates, seeking the threshold that optimizes the solution by searching for the solution of minimum energy.</p> <p></p> <p>This algorithm, therefore, allows selecting the most important data to generate a viable solution, discarding the excess data that cannot produce the required solution for a specific task. This enables the collection of a large amount of diverse data within the same workspace, with the algorithm itself choosing those that yield a more efficient solution, eliminating the need to manually select which data to use in each learning process.</p>"},{"location":"Manipulation/gmm.html#papers-and-publications","title":"Papers and Publications","text":"<ul> <li>f-Divergence Optimization for Task-Parameterized Learning from Demonstrations Algorithm</li> </ul>"},{"location":"Manipulation/gmm.html#learning-and-generalization-of-task-parameterized-skills-through-few-human-demonstrations","title":"Learning and generalization of task-parameterized skills through few human demonstrations","text":"<p>In this paper, we introduce a groundbreaking concept that enriches the original training dataset with synthetic data, thereby enabling significant improvements in policy learning. Consequently, this novel approach empowers the acquisition of task-parameterized skills with a limited number of demonstrations, paving the way for enhanced practicality. The final result of this work is to present a real application in the field of aid to dependent persons. For this purpose, tests have been carried out for the task of sweeping an area by our manipulator robot ADAM, through a few demonstrations captured by a human demonstrator.</p> <p></p> <p>This algorithm, therefore, allows us to learn complex tasks with minimal demonstration data, thus overcoming one of the limitations of GMMs. With the developed algorithm, we can generate synthetic solutions based on the limited initial information. These data are at least as effective as those provided by the user.</p> <p></p>"},{"location":"Manipulation/gmm.html#papers-and-publications_1","title":"Papers and Publications","text":"<ul> <li> <p>Learning and generalization of task-parameterized skills through few human demonstrations</p> </li> <li> <p>Learning from demonstration through synthetic data for parameterized tasks</p> </li> </ul>"},{"location":"Manipulation/gmm.html#reorganization-of-elements-in-cluttered-environments","title":"Reorganization of elements in cluttered environments","text":"<p>The rearrangement of objects is an essential task in daily human life. Subconsciously, humans break down such tasks into three components: perception, reasoning, and execution, which are automatically resolved. This process represents a significant challenge for robots, as it must apply complex logic to treat all the information and successfully execute the task. In this research, we propose a solution to perform this task in a human-like manner. For that purpose, we developed a modular algorithm that provides the capability to observe and understand the scene, imaginate the best solution and execute it, following human-like reasoning. This is done combining a zero-shot deep learning model for perception, a zero-shot large diffusion model to provide a human-like final scene and a Learning from Demonstration algorithm for execution. To test the performance, we have made several experiments to check the correct resolution of the rearrangement task. To this end, we have checked the efficiency of the final scene generated, the correct performance of the path using human demonstrations and finally experiments with two different robots in a simulated and real environment. The results obtained prove the adaptability of our algorithm to different environments, objects and robots.</p> <p></p>"},{"location":"Manipulation/gmm.html#papers-and-publications_2","title":"Papers and Publications","text":"<ul> <li>Everyday Objects Rearrangement in a Human-Like Manner via Robotic Imagination and Learning From Demonstration</li> </ul>"},{"location":"Manipulation/gps.html","title":"Gaussian Process Learning Methods","text":""},{"location":"Manipulation/gps.html#gaussian-process-learning-methods","title":"Gaussian Process Learning Methods","text":"<p>Gaussian processes (GPs) have become a powerful tool in LfD for robotic manipulation due to their ability to model complex, non-linear functions in a probabilistic framework. In LfD, robots learn tasks by observing human demonstrations, and GPs provide a flexible and efficient way to capture the underlying structure of the demonstrated movements. GPs are particularly useful for modeling continuous trajectories and encoding uncertainty, which is crucial for tasks requiring precision and adaptability. By leveraging the ability of GPs to make predictions with confidence intervals, robots can generate smooth and accurate trajectories while accounting for variations in the demonstrations. This approach has been successfully applied in robotic manipulation, where GPs enable robots to generalize learned skills and adapt to new, previously unseen situations, improving performance and robustness in real-world environments.</p>"},{"location":"Manipulation/gps.html#learning-of-movement-primitives-by-gaussian-processes-from-demonstrations","title":"Learning of Movement Primitives by Gaussian Processes from Demonstrations","text":"<p>Currently, the use of Learning from Demonstration (LfD) techniques has proven effective for encoding human skills to solve specific tasks. Among all existing techniques, the use of algorithms based on movement primitives presents an effective way to encode basic robot movements. Most of these techniques learn through parametric approaches, which increases the demands for human effort and also limits reproduction accuracy. Additionally, many LfD techniques focus on working in static environments, without considering how the skill might change in the presence of obstacles in the environment. In this work, we present a non-parametric movement primitive generation algorithm based on the use of Gaussian Processes (GP), called Gaussian Movement Primitive (GMP). Unlike other techniques, our algorithm has a sufficient condition to ensure that the generated trajectory can pass through the desired points with 100% probability, as well as the ability to be combined analytically and to avoid obstacles that may appear in the environment. To verify its efficiency, comparisons have been made in simulations on the LASA and RAIL datasets against other state-of-the-art algorithms to show the advantages of the presented algorithm, as well as experiments in real environments with a robotic arm. The developed method allows working both in the N-dimensional joint space and in the Cartesian space.</p> <p></p> <p>Our new method for non-parametric movement primitive learning based on Gaussian Processes, is called Gaussian Movement Primitives (GMP). The method combines some of the previously presented ideas on movement primitive learning through the use of GPs~\\cite{jin2023gaussian,arduengo2023gaussian} and adds the capability to consider dynamic elements of the environment, such as obstacles. Additionally, the proposed algorithm employs a temporal alignment process that allows it to work with demonstrations at different speeds. The algorithm is also equipped with an initial processing of the demonstrations that not only avoids the manual selection of hyperparameters but also adjusts its uncertainty to the specific demonstrations of each task, a factor that is highly relevant for achieving optimal results. With all these characteristics, the algorithm can generate satisfactory task reproductions, adapt to dynamic environments, and operate efficiently in N-dimensional environments,features that the algorithms presented in this work do not possess. The next figure present the solution of the algortihm using 3D data in Cartesian space. Can be seen how the algorithm takes into account obstacles of the environment and generates a trajectory that avoids them.</p> <p></p>"},{"location":"Navigation/demo.html","title":"In work :construction_worker: :construction:","text":""},{"location":"Simulation/ADAMSim.html","title":"ADAM Simulator","text":""},{"location":"Simulation/ADAMSim.html#adam-simulator","title":"ADAM SIMULATOR","text":"<p>ADAM is equipped with a simulation environment that allows it to test its algorithms in a virtual environment before deploying them in the real world. This section details the proposed method to generate random domestic environments in the PyBullet simulator. The objective is to generate environments where we can tests our algorithms and check whether the simulated models are valid for use in the real robot. </p> <p>These simulator is still in development, but it is intended to be a realistic simulation of the ADAM robot and establish a connection with the physical robot.</p> <p>New information will be added in a near future.</p>"},{"location":"Simulation/Indoor.html","title":"Indoor Environments","text":""},{"location":"Simulation/Indoor.html#simulation-of-indoor-environments","title":"SIMULATION OF INDOOR ENVIRONMENTS","text":"<p>A simulation of domestic environments has been developed in order to have a digital model that allows the development of the different algorithms. In addition, it is also intended to acquire data from the environment to retrain neural networks and check whether the simulated models are valid for use in the real robot. </p>"},{"location":"Simulation/Indoor.html#simulators-for-robotics","title":"SIMULATORS FOR ROBOTICS","text":"<p>The options available in terms of commercial simulators are very varied. Technological advances and constant change in the field mean that new simulators are appearing, while others are becoming obsolete. There is no guide that facilitates the selection of a suitable simulation tool for the specific needs of each researcher. Some of the most prominent simulators in the field of robotics are presented below, detailing their characteristics:</p> SIMULATOR CHRONOS COPPELIASIM GAZEBO ISAAC UNITY WEBOTS GPS V V V V V V LIDAR V V V V V V TRACKS V V V V V V WHEELS V V V V V V OMNI WHEELS V V V V V V HEIGHTMAP IMPORT V V V V V V OPENDRIVE X X X X X V OPENSTREETMAP X X X X X V PATHPLANNING X V V V X V ROS SUPPORT X V V V X V RGBD V V V V V V REALISTIC RENDERING V V X V V X"},{"location":"Simulation/Indoor.html#methodology","title":"METHODOLOGY","text":"<p>This section details the proposed method to generate random domestic environments in the CoppeliaSim simulator. The objective is to generate environment containing realistic household elements ensuring a logical room distribution, and where a robot, in our case the ADAM, can be introduced to operate and perform different applications. </p>"},{"location":"Simulation/Indoor.html#generation-of-random-domestic-environments","title":"GENERATION OF RANDOM DOMESTIC ENVIRONMENTS","text":"<p>The domestic environment is modeled as a 3x3 matrix  divided into cells  for  which correspond to a certain area. This ensures a regular house plan that can allocate diverse essential room types (such as kitchens or bathrooms) and wide open spaces. Both the orientation and position of a room must be indicated when generating a room in the simulator. Rooms, and specially the outermost ones divided into corner rooms in contact with two rooms, , side rooms in contact with three rooms,  and center room (in contact with four rooms, N=4), must be correctly oriented to avoid placing doors that coincide with external or internal walls. For each cell  we define a set of connection vectors , where each connection  where , points to the location of an adjacent room. Then, each room model must be rotated to the orientation  for which the room's connections  are aligned with cell connections  of their assigned location. This means that in their proper orientation, the room's connections point to existing rooms, not out of bounds.</p> <p></p> <p>This process is divided in two steps:</p> <ul> <li>Random selection of room: The first part of the method consists on the random selection of room models for each type of room. To generate our environments, each type of room is generated once. Table below shows the different room types with their default connection vectors to adjacent rooms at . Note that type 4 rooms that correspond to living room models are described as large rooms. This type of room occupies two adjacent areas (cells) in any orientation, i.e.,  or . Type 5 room models are designated to be the center area with fixed position .</li> </ul> TYPE DESCRIPTION CONNECTIONS AT  0 Hall (Side) [[0, -1],[-1, 0],[0,1]] 1 Kitchen (Corner) [[0, -1],[-1, 0]] 2 Bathroom (Corner) [[0, -1],[-1, 0]] 3 Free use (Side) [[0, -1],[-1, 0],[0,1]] 4 Living room (Side, large) [[0, -1],[-1, 0],[0,1]] 5 Center area [[0, -1],[-1, 0],[0,1],[1,0]] 6 Bedroom (Corner) [[0, -1],[-1, 0]] 7 Office (Side) [[0, -1],[-1, 0],[0,1]] <p>A set of  distinct room models in CoppeliaSim are predefined for each room type , so the probability of a model  being selected for a certain type is . Figures below shows an example of room models for side and corner rooms, oriented at  and located at the world origin. The result of this part is a set of unique room models identified by their room type  initialized at default position, origin and connections.</p> <ul> <li>Random distribution of rooms:  The second part of the method consist in the random distribution of the rooms. We define a vector  where each of its elements  contains the randomly selected room model  and its corresponding connections  for each room type . The elements in  are randomly shuffled and assigned to empty cells in the area distribution matrix  according to their room type. It is also ensured that the room connections  align with their corresponding cell connections (pointing to not out of bounds existing areas) and rotated otherwise with a rotation matrix  and . Other restrictions imposed are the location of the center room in cell  and the assignation of two adjacent cells to large rooms as previously mentioned in this Section.</li> </ul> <p>Having assigned the rooms to each area cell, the models are placed and oriented in the empty scene in CoppeliaSim to which the algorithm is connected. The models are placed at the coordinates given by  times the room length of a cell  m and oriented at their corresponding angle  with respect to the world's reference frame.</p> <p>The algorithm is not only responsible for placing the rooms in certain positions, but also for orienting them in such a way that they do not generate discontinuities or failures in the design.</p> <p></p>"},{"location":"Simulation/Indoor.html#examples-of-the-rooms","title":"EXAMPLES OF THE ROOMS","text":"<p>The following are examples of individually designed rooms for the generation of a complete environment.</p> <p> </p> <p>Several complete simulation environments have been generated where the rooms comply with the appropriate positions and orientations indicated in the matrix.</p> <p></p>"},{"location":"Simulation/Indoor.html#paper-associated","title":"PAPER ASSOCIATED","text":"<p>This work is fully presented in the following paper:</p> <p>Data Generation in Simulated Domestic Environments for Assistive Robots</p>"},{"location":"Vision/demo.html","title":"In work :construction_worker: :construction:","text":""},{"location":"about/about.html","title":"About Us","text":""},{"location":"about/about.html#about-us","title":"About Us","text":"<p>The group is composed of various PhD candidates, master's students, and doctors who are responsible for developing the different algorithms and tasks on our robot to perform various operations in real-world environments. The group is led by the following people:</p>"},{"location":"about/about.html#group-leaders","title":"Group Leaders","text":"Research List <p> Ram\u00f3n Barber is an Associate Professor of the System Engineering and Automation Department, at the University Carlos III of Madrid, Spain. He received the B.Sc. degree in Industrial Engineering from Polytechnic University of Madrid (1995), and the Ph. D. degree in Industrial Technologies from the University Carlos III (2000). His research topics are focused on Mobile Robotics including perception of the environment, environment modelling, planning, localization and navigation tasks, considering geometrical, topological and semantic representations. He is a member of the International Federation of Automatic Control (IFAC) and of the IEEE. </p> Research List <p> Luis Moreno is PhD in Industrial Engineering (1988) from Univ. Politecnica de Madrid. From 1994 is Professor at University Carlos III of Madrid, and from 2009 Full Professor. His main research line is Robotics. He has been involved in different research projects in mobile manipulators, lightweight robots, advanced actuators, mobile robots, path planning, perception, navigation, environment modeling and exoskeletons. He has participated in more than 40 research projects with public funding and 26 projects with private funding. He is author of 4 patents, 5 books, 18 book chapters, 80 Journal papers, and 160 conference papers. He has been supervisor of 28 PhD theses. </p> Research List <p> Santiago Garrido received the degree in mathematics from the Complutense University of Madrid, in 1979, and the degree in physics and the Ph.D. degree from the Universidad Carlos III de Madrid, Madrid, Spain, in 1955 and 2000, respectively. In 1997, he joined the Department of Systems Engineering and Automation, Universidad Carlos III de Madrid, where he has been involved in several mobile robotics projects. His research interests include mobile robotics, mobile manipulators, environment modeling, path planning, and mobile robot global localization problems. </p>"},{"location":"about/about.html#researchers","title":"Researchers","text":"Research List <p> Alicia Mora was born in Getafe, Spain, in 1998. She received the B.S. degree in Industrial Electronics and Automation Engineering (2020) and the M.S. in Robotics and Automation (2022) at Carlos III University of Madrid. She is currently pursuing her PhD and collaborating as a researcher with Robotics Lab at University Carlos III of Madrid. Her main research topics are focused on mobile robots, including mapping considering geometric, topological and semantic representations, as well as navigation for assistive robots. </p> Research List <p> Adri\u00e1n Prados was born in Legan\u00e9s, Spain, in 1999. He received the B.S. degree in Industrial Electronics and Automation Engineering (2021) and the M.S. in Robotics and Automation (2023) at University Carlos III of Madrid (UC3M). He is currently pursuing his PhD in the program of Electrical, Electronic and Automation Engineering and collaborating as a researcher with Robotics Lab at University Carlos III of Madrid. His main research topics are focused on Imitation Learning and Learning from Demonstration, including generalization of learned tasks and adaptation to new environments, applied to manipulation tasks. He also works on control of mobile manipulators, deep learning, optimization techniques, and assistive robots. </p> Research List <p> Alberto M\u00e9ndez was born in Santa Cruz de Tenerife, Spain, in 1999. He received the B.S. degree in Electronics and Automatic Engineering at University of La Laguna (ULL) in 2021 and the M.S. in Robotics and Automation at University Carlos III of Madrid (UC3M) in 2023. Currently, Alberto Mendez is a PhD candidate in the program of Electrical, Electronic and Automation Engineering at UC3M. Also, he is collaborating as a researcher with RoboticsLab at University Carlos III of Madrid. His lines of research focus on computer vision and deep learning. Including topics such as the detection and localisation of elements in the environment, artificial intelligence and 3D perception. </p> Research List <p> Noelia Fern\u00e1ndez is a Technical Industrial Engineer. She has completed the Master's Degree in Industrial Engineering and is currently finishing her studies in the Master's Degree in Robotics and Automation. She started designing and programming an autonomous terrestrial robot for interventions with firefighters from scratch. Later, she explored other fields such as manipulation with industrial robotic arms coordinated with vision systems, navigation and environment monitoring, development of indoor simulations, and training of neural networks for object detection. Currently, she is developing HRI systems integrating AI with other types of information to achieve applications to improve people's lives. </p> Research List <p> Gonzalo Espinoza was born in Getafe, Spain, in 1998. He received the B.S. degree in Industrial Electronics and Automation Engineering (2020) and the M.S. in Robotics and Automation (2022) at Carlos III University of Madrid. She is currently pursuing her PhD and collaborating as a researcher with Robotics Lab at University Carlos III of Madrid. Her main research topics are focused on mobile robots, including mapping considering geometric, topological and semantic representations, as well as navigation for assistive robots. </p>"}]}